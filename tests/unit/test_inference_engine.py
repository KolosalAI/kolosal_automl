import pytest
import numpy as np
import os
import pickle
import json
import time
import tempfile
from unittest.mock import patch, MagicMock, Mock
import threading
from concurrent.futures import Future

# Import the classes we're testing
try:
    from modules.engine.inference_engine import (
        InferenceEngine, 
        DynamicBatcher,
        MemoryPool,
        PerformanceMetrics,
        PredictionRequest, 
        BatchPriority
    )
    from modules.configs import (
        InferenceEngineConfig,
        ModelType,
        EngineState,
        QuantizationConfig,
        BatchProcessorConfig,
        BatchProcessingStrategy,
        PreprocessorConfig,
        NormalizationType,
        QuantizationMode
    )
except ImportError as e:
    pytest.skip(f"Required modules not available: {e}", allow_module_level=True)


# Test PredictionRequest class
@pytest.mark.unit
class TestPredictionRequest:
    """Test the PredictionRequest class"""
    
    def test_prediction_request_creation(self):
        """Test creating a prediction request"""
        features = np.array([[1, 2, 3], [4, 5, 6]])
        request = PredictionRequest(
            id="test-123",
            features=features,
            priority=1,
            timestamp=123.45,
            future=None,
            timeout_ms=1000.0
        )
        
        assert request.id == "test-123"
        assert np.array_equal(request.features, features)
        assert request.priority == 1
        assert request.timestamp == 123.45
        assert request.future is None
        assert request.timeout_ms == 1000.0
    
    def test_prediction_request_priority_comparison(self):
        """Test priority comparison between requests"""
        # Lower priority value means higher priority
        high_priority = PredictionRequest(id="high", features=np.array([1]), priority=0, timestamp=100.0)
        medium_priority = PredictionRequest(id="medium", features=np.array([2]), priority=1, timestamp=100.0)
        low_priority = PredictionRequest(id="low", features=np.array([3]), priority=2, timestamp=100.0)
        
        # Compare priorities
        assert high_priority < medium_priority
        assert medium_priority < low_priority
        assert high_priority < low_priority
        
        # Same priority but different timestamps
        older_req = PredictionRequest(id="older", features=np.array([4]), priority=1, timestamp=100.0)
        newer_req = PredictionRequest(id="newer", features=np.array([5]), priority=1, timestamp=200.0)
        
        # Older request should be processed first (lower timestamp = higher priority)
        assert older_req < newer_req


@pytest.mark.unit
class TestMemoryPool:
    """Test the MemoryPool class"""
    
    @pytest.fixture(autouse=True)
    def setup_memory_pool(self):
        """Setup memory pool for each test"""
        self.memory_pool = MemoryPool(max_buffers=5)
    
    def test_get_buffer(self):
        """Test getting a buffer from the pool"""
        shape = (10, 5)
        buffer = self.memory_pool.get_buffer(shape)
        
        assert buffer.shape == shape
        assert buffer.dtype == np.float32
        assert np.all(buffer == 0)  # Should be zeroed
    
    def test_return_buffer(self):
        """Test returning a buffer to the pool and reusing it"""
        shape = (8, 4)
        
        # Get a buffer, modify it, and return it
        buffer1 = self.memory_pool.get_buffer(shape)
        buffer1.fill(42)
        self.memory_pool.return_buffer(buffer1)
        
        # Get another buffer with the same shape - should be the same one, but zeroed
        buffer2 = self.memory_pool.get_buffer(shape)
        
        # In real implementation we'd expect buffer1 and buffer2 to be the same object
        # but zeroed. Due to implementation details, we'll just test that it's zeroed.
        self.assertTrue(np.all(buffer2 == 0))
    
    def test_pool_capacity(self):
        """Test that the pool doesn't exceed its capacity"""
        shape = (5, 5)
        buffers = []
        
        # Fill the pool to capacity + 1
        for _ in range(self.memory_pool.max_buffers + 1):
            buffer = self.memory_pool.get_buffer(shape)
            buffer.fill(1)  # Mark the buffer
            buffers.append(buffer)
        
        # Return all buffers to the pool
        for buffer in buffers:
            self.memory_pool.return_buffer(buffer)
        
        # Get stats and check count
        stats = self.memory_pool.get_stats()
        # Only max_buffers should be in the pool, not max_buffers+1
        assert stats["total_buffers"] == self.memory_pool.max_buffers
    
    def test_clear_pool(self):
        """Test clearing the pool"""
        # Add some buffers to the pool
        shape = (3, 3)
        buffer = self.memory_pool.get_buffer(shape)
        self.memory_pool.return_buffer(buffer)
        
        # Clear the pool
        self.memory_pool.clear()
        
        # Check that pool is empty
        stats = self.memory_pool.get_stats()
        assert stats["total_buffers"] == 0


@pytest.mark.unit
class TestPerformanceMetrics:
    """Test the PerformanceMetrics class"""
    
    @pytest.fixture(autouse=True)
    def setup_metrics(self):
        """Setup performance metrics for each test"""
        self.metrics = PerformanceMetrics(window_size=3)  # Small window for testing
    
    def test_update_inference(self):
        """Test updating inference metrics"""
        # Add some metrics
        self.metrics.update_inference(0.1, 10, 0.01, 0.02)
        self.metrics.update_inference(0.2, 20, 0.02, 0.03)
        
        # Check metrics calculation
        metrics_dict = self.metrics.get_metrics()
        
        assert self.metrics.total_requests == 30  # 10 + 20
        assert abs(metrics_dict["avg_inference_time_ms"] - 150.0) < 0.1  # (0.1 + 0.2) / 2 * 1000
        assert abs(metrics_dict["avg_preprocessing_time_ms"] - 15.0) < 0.1  # (0.01 + 0.02) / 2 * 1000
        assert abs(metrics_dict["avg_quantization_time_ms"] - 25.0) < 0.1  # (0.02 + 0.03) / 2 * 1000
    
    def test_window_limit(self):
        """Test that metrics window is limited"""
        # Fill window beyond capacity
        for i in range(5):  # Window size is 3
            self.metrics.update_inference(float(i), 1)
        
        # Only the last 3 values should be in the window
        assert len(self.metrics.inference_times) == 3
        assert self.metrics.inference_times == [2.0, 3.0, 4.0]
    
    def test_error_recording(self):
        """Test recording errors"""
        # Record errors
        for _ in range(3):
            self.metrics.record_error()
        
        metrics_dict = self.metrics.get_metrics()
        assert self.metrics.total_errors == 3
        assert metrics_dict["total_errors"] == 3
    
    def test_cache_hits_misses(self):
        """Test recording cache hits and misses"""
        # Record cache activity
        for _ in range(5):
            self.metrics.record_cache_hit()
        
        for _ in range(5):
            self.metrics.record_cache_miss()
        
        metrics_dict = self.metrics.get_metrics()
        assert metrics_dict["cache_hit_rate"] == 0.5  # 5 hits / (5 hits + 5 misses)


@pytest.mark.unit
class TestDynamicBatcher:
    """Test the DynamicBatcher class"""
    
    @pytest.fixture(autouse=True)
    def setup_batcher(self):
        """Setup dynamic batcher for each test"""
        # Create a mock batch processor function
        self.batch_results = None
        
        def mock_processor(features):
            # Simulate processing by returning the sum of each row
            result = np.sum(features, axis=1).reshape(-1, 1)
            self.batch_results = result
            return result
        
        self.mock_processor = mock_processor
        
        # Create the batcher
        self.batcher = DynamicBatcher(
            batch_processor=self.mock_processor,
            max_batch_size=2,  # Small for testing
            max_wait_time_ms=50.0,  # Short for testing
            max_queue_size=10
        )
        
        # Start the batcher
        self.batcher.start()
    
    def tearDown(self):
        # Stop the batcher
        self.batcher.stop()
    
    def test_enqueue_and_process(self):
        """Test enqueuing requests and processing them"""
        # Create futures for testing
        from concurrent.futures import Future
        future1 = Future()
        future2 = Future()
        
        # Create test requests
        features1 = np.array([[1, 2, 3]])
        features2 = np.array([[4, 5, 6]])
        
        request1 = PredictionRequest(
            id="req1",
            features=features1,
            priority=1,
            timestamp=time.monotonic(),
            future=future1
        )
        
        request2 = PredictionRequest(
            id="req2",
            features=features2,
            priority=1,
            timestamp=time.monotonic(),
            future=future2
        )
        
        # Enqueue requests
        self.assertTrue(self.batcher.enqueue(request1))
        self.assertTrue(self.batcher.enqueue(request2))
        
        # Wait for processing (should batch and process together)
        result1 = future1.result(timeout=0.5)
        result2 = future2.result(timeout=0.1)  # Short since both should be done
        
        # Check results
        self.assertIsNotNone(result1)
        self.assertIsNotNone(result2)
        self.assertEqual(result1[0][0], 6)  # 1+2+3
        self.assertEqual(result2[0][0], 15)  # 4+5+6
    
    def test_request_priority(self):
        """Test that high priority requests get processed first"""
        # This is a timing-sensitive test that may be flaky
        from concurrent.futures import Future
        
        # Create a high priority request that comes later but should be processed first
        high_priority_future = Future()
        low_priority_future = Future()
        
        # Create test requests - low priority first
        low_priority = PredictionRequest(
            id="low",
            features=np.array([[1, 1, 1]]),
            priority=10,  # Lower priority (higher value)
            timestamp=time.monotonic(),
            future=low_priority_future
        )
        
        # Enqueue low priority request
        self.batcher.enqueue(low_priority)
        
        # Short delay
        time.sleep(0.01)
        
        # Now add high priority request
        high_priority = PredictionRequest(
            id="high",
            features=np.array([[2, 2, 2]]),
            priority=0,  # Higher priority (lower value)
            timestamp=time.monotonic(),
            future=high_priority_future
        )
        
        # Enqueue high priority request and check it gets processed quickly
        self.batcher.enqueue(high_priority)
        
        # Both should complete eventually
        high_result = high_priority_future.result(timeout=0.5)
        low_result = low_priority_future.result(timeout=0.1)
        
        self.assertEqual(high_result[0][0], 6)  # 2+2+2
        self.assertEqual(low_result[0][0], 3)   # 1+1+1
        
    def test_max_queue_size(self):
        """Test that queue size limits are enforced"""
        # Fill queue to capacity
        futures = []
        for i in range(self.batcher.max_queue_size):
            future = Future()
            futures.append(future)
            
            request = PredictionRequest(
                id=f"req{i}",
                features=np.array([[i, i, i]]),
                priority=1,
                timestamp=time.monotonic(),
                future=future
            )
            
            # Should be able to enqueue up to max_queue_size
            self.assertTrue(self.batcher.enqueue(request))
        
        # One more request should fail
        future_extra = Future()
        request_extra = PredictionRequest(
            id="extra",
            features=np.array([[99, 99, 99]]),
            priority=1,
            timestamp=time.monotonic(),
            future=future_extra
        )
        
        # Should reject when queue is full
        assert not self.batcher.enqueue(request_extra)


@pytest.mark.unit
@patch('modules.engine.inference_engine.LRUTTLCache')
@patch('modules.engine.inference_engine.MemoryPool')
@patch('modules.engine.inference_engine.DataPreprocessor')
@patch('modules.engine.inference_engine.Quantizer')
class TestInferenceEngine:
    """Test the InferenceEngine class with mocked dependencies"""
    
    @pytest.fixture(autouse=True)
    def setup_engine(self):
        """Setup inference engine for each test"""
        # Create a simple linear model for testing
        try:
            from sklearn.linear_model import LinearRegression
            self.model = LinearRegression()
            self.model.fit(np.array([[1], [2], [3]]), np.array([2, 4, 6]))
        except ImportError:
            # Create a mock model if sklearn is not available
            self.model = Mock()
            self.model.predict.return_value = np.array([2.0, 4.0, 6.0])
            self.model.feature_names_in_ = ["feature1"]
            self.model.n_features_in_ = 1
        
        # Create temp file to save model
        self.temp_dir = tempfile.mkdtemp()
        self.temp_file_path = os.path.join(self.temp_dir, 'model.pkl')
        
        with open(self.temp_file_path, 'wb') as f:
            pickle.dump(self.model, f)
        
        # Create minimal config for testing
        self.config = InferenceEngineConfig(
            num_threads=2,
            enable_batching=False,
            enable_feature_scaling=False,
            enable_quantization=False,
            enable_request_deduplication=False,
            enable_monitoring=False,
            debug_mode=True  # For more logging
        )
        
        # Setup cleanup
        yield
        
        # Cleanup (equivalent to tearDown)
        if hasattr(self, 'engine') and self.engine is not None:
            self.engine.shutdown()
            self.engine = None
        
        # Remove temp model file and directory
        try:
            if os.path.exists(self.temp_file_path):
                os.unlink(self.temp_file_path)
            if os.path.exists(self.temp_dir):
                os.rmdir(self.temp_dir)
        except (PermissionError, OSError) as e:
            print(f"Warning: Failed to clean up temporary file: {e}")
    
    def test_initialization(self, mock_Quantizer, mock_DataPreprocessor, mock_MemoryPool, mock_LRUTTLCache):
        """Test engine initialization"""
        # Create cached and mocked objects
        mock_MemoryPool.return_value = MagicMock()
        
        # Initialize engine
        self.engine = InferenceEngine(self.config)
        
        # Check initial state
        assert self.engine.state == EngineState.READY
        assert self.engine.model is None
        assert self.engine.active_requests == 0
        
        # Make sure thread safety objects are created
        assert self.engine.inference_lock is not None
        assert self.engine.state_lock is not None
    
    def test_load_model(self, mock_Quantizer, mock_DataPreprocessor, mock_MemoryPool, mock_LRUTTLCache):
        """Test loading a model"""
        # Create cached and mocked objects
        mock_MemoryPool.return_value = MagicMock()
        
        # Initialize engine
        self.engine = InferenceEngine(self.config)
        
        # Load the model
        success = self.engine.load_model(self.temp_file_path, model_type=ModelType.SKLEARN)
        
        # Check result
        assert success
        assert self.engine.state == EngineState.READY
        assert self.engine.model is not None
        assert self.engine.model_type == ModelType.SKLEARN
        
        # Model info should be populated
        self.assertIn("model_type", self.engine.model_info)
        assert self.engine.model_info["model_type"] == "SKLEARN"
    
    def test_predict(self, mock_Quantizer, mock_DataPreprocessor, mock_MemoryPool, mock_LRUTTLCache):
        """Test making predictions"""
        # Create cached and mocked objects
        mock_MemoryPool.return_value = MagicMock()
        
        # Initialize engine
        self.engine = InferenceEngine(self.config)
        
        # Load the model
        success = self.engine.load_model(self.temp_file_path, model_type=ModelType.SKLEARN)
        assert success
        
        # Make a prediction
        features = np.array([[4.0]])
        success, predictions, metadata = self.engine.predict(features)
        
        # Check result
        assert success
        assert predictions is not None
        assert predictions[0] == 8.0  # 4*2 from our linear model
        assert "inference_time_ms" in metadata
    
    @patch('modules.engine.inference_engine.DynamicBatcher')
    def test_batched_prediction(self, mock_DynamicBatcher, mock_Quantizer, mock_DataPreprocessor, 
                              mock_MemoryPool, mock_LRUTTLCache):
        """Test batched prediction path"""
        # Configure the dynamic batcher mock
        mock_batcher = MagicMock()
        mock_DynamicBatcher.return_value = mock_batcher
        
        # Create an engine with batching enabled
        config = self.config
        config.enable_batching = True
        
        # Initialize engine
        self.engine = InferenceEngine(config)
        
        # Load the model
        success = self.engine.load_model(self.temp_file_path, model_type=ModelType.SKLEARN)
        assert success
        
        # Prepare to enqueue a prediction
        features = np.array([[4.0]])
        future = MagicMock()
        mock_batcher.enqueue.return_value = True
        
        # Make async prediction
        result_future = self.engine.enqueue_prediction(features, priority=BatchPriority.HIGH)
        
        # Check that it called enqueue on the batcher
        mock_batcher.enqueue.assert_called_once()
        assert mock_batcher.enqueue.call_args[0][0].features.tolist() == features.tolist()
        assert mock_batcher.enqueue.call_args[0][0].priority == BatchPriority.HIGH.value
    
    def test_memory_pool_integration(self, mock_Quantizer, mock_DataPreprocessor, mock_MemoryPool, mock_LRUTTLCache):
        """Test integration with memory pool"""
        # Use real memory pool
        mock_MemoryPool.side_effect = None
        
        # Initialize engine
        self.engine = InferenceEngine(self.config)
        
        # Check that memory pool was created
        assert self.engine.memory_pool is not None
        
        # Test getting metrics
        metrics = self.engine.get_performance_metrics()
        assert "memory_pool" in metrics
    
    def test_validation(self, mock_Quantizer, mock_DataPreprocessor, mock_MemoryPool, mock_LRUTTLCache):
        """Test model validation"""
        # Mock memory pool
        mock_MemoryPool.return_value = MagicMock()
        
        # Initialize engine
        self.engine = InferenceEngine(self.config)
        
        # Validate without model
        validation = self.engine.validate_model()
        assert not validation['valid']
        
        # Load model and validate
        self.engine.load_model(self.temp_file_path, model_type=ModelType.SKLEARN)
        validation = self.engine.validate_model()
        
        # Should be valid now
        assert validation['valid']
        assert validation['model_type'] == 'SKLEARN'
    
    def test_shutdown(self, mock_Quantizer, mock_DataPreprocessor, mock_MemoryPool, mock_LRUTTLCache):
        """Test engine shutdown"""
        # Mock objects
        mock_memory_pool = MagicMock()
        mock_MemoryPool.return_value = mock_memory_pool
        
        # Initialize engine
        self.engine = InferenceEngine(self.config)
        
        # Load model
        self.engine.load_model(self.temp_file_path, model_type=ModelType.SKLEARN)
        
        # Shutdown
        self.engine.shutdown()
        
        # Check state
        assert self.engine.state == EngineState.STOPPED
        
        # Check that resources were released
        mock_memory_pool.clear.assert_called_once()
        assert self.engine.model is None
        assert self.engine.compiled_model is None


# Additional mocks needed for the test suite
class MockBatchProcessor:
    def __init__(self, config):
        self.config = config
        
    def start(self, process_func):
        self.process_func = process_func
        
    def submit_batch(self, batch):
        return self.process_func(batch)


if __name__ == "__main__":
    pytest.main([__file__])