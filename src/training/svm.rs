//! Support Vector Machine implementations
//!
//! Provides SVM classifier and regressor using SMO (Sequential Minimal Optimization) algorithm.

use crate::error::{KolosalError, Result};
use ndarray::{Array1, Array2};
use rayon::prelude::*;
use rand::prelude::*;
use rand_xoshiro::Xoshiro256PlusPlus;
use serde::{Deserialize, Serialize};

/// Maximum number of samples for eager kernel matrix computation.
/// Beyond this, training will return an error to prevent OOM.
const MAX_KERNEL_MATRIX_SAMPLES: usize = 10_000;

/// Kernel function type
#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub enum KernelType {
    /// Linear kernel: K(x, y) = x · y
    Linear,
    /// Polynomial kernel: K(x, y) = (γ * x · y + r)^d
    Polynomial { degree: usize, gamma: f64, coef0: f64 },
    /// Radial Basis Function (Gaussian): K(x, y) = exp(-γ * ||x - y||²)
    RBF { gamma: f64 },
    /// Sigmoid kernel: K(x, y) = tanh(γ * x · y + r)
    Sigmoid { gamma: f64, coef0: f64 },
}

impl Default for KernelType {
    fn default() -> Self {
        KernelType::RBF { gamma: 1.0 }
    }
}

/// SVM configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SVMConfig {
    /// Regularization parameter (C)
    pub c: f64,
    /// Kernel function
    pub kernel: KernelType,
    /// Tolerance for stopping criterion
    pub tol: f64,
    /// Maximum number of iterations
    pub max_iter: usize,
    /// Random seed
    pub random_state: Option<u64>,
    /// Epsilon for regression (SVR tube width)
    pub epsilon: f64,
}

impl Default for SVMConfig {
    fn default() -> Self {
        Self {
            c: 1.0,
            kernel: KernelType::RBF { gamma: 1.0 },
            tol: 1e-3,
            max_iter: 1000,
            random_state: Some(42),
            epsilon: 0.1,
        }
    }
}

/// A single binary SVM trained for one class vs rest
#[derive(Debug, Clone, Serialize, Deserialize)]
struct BinarySVM {
    support_vectors: Array2<f64>,
    alphas: Array1<f64>,
    support_labels: Array1<f64>,
    bias: f64,
}

/// Support Vector Classifier
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SVMClassifier {
    config: SVMConfig,
    /// Support vectors (used for binary classification)
    support_vectors: Option<Array2<f64>>,
    /// Alpha coefficients (Lagrange multipliers)
    alphas: Option<Array1<f64>>,
    /// Support vector labels
    support_labels: Option<Array1<f64>>,
    /// Bias term
    bias: f64,
    /// Unique class labels
    classes: Vec<i64>,
    /// One-vs-Rest binary classifiers for multi-class
    ovr_classifiers: Vec<BinarySVM>,
    is_fitted: bool,
}

impl SVMClassifier {
    /// Create a new SVM classifier
    pub fn new(config: SVMConfig) -> Self {
        Self {
            config,
            support_vectors: None,
            alphas: None,
            support_labels: None,
            bias: 0.0,
            classes: Vec::new(),
            ovr_classifiers: Vec::new(),
            is_fitted: false,
        }
    }

    /// Fit the classifier (supports binary and multi-class via One-vs-Rest)
    pub fn fit(&mut self, x: &Array2<f64>, y: &Array1<f64>) -> Result<()> {
        // Validate that all labels are integral values (no silent truncation)
        for (i, &v) in y.iter().enumerate() {
            if (v - v.round()).abs() > 1e-9 {
                return Err(KolosalError::InvalidInput(
                    format!("SVM classifier requires integer class labels, but sample {} has label {}", i, v)
                ));
            }
        }

        let mut classes: Vec<i64> = y.iter().map(|&v| v.round() as i64).collect();
        classes.sort();
        classes.dedup();

        if classes.len() < 2 {
            return Err(KolosalError::InvalidInput(
                "SVM requires at least 2 distinct classes".to_string()
            ));
        }

        self.classes = classes.clone();

        if classes.len() == 2 {
            self.fit_binary(x, y)?;
        } else {
            self.fit_ovr(x, y)?;
        }

        self.is_fitted = true;
        Ok(())
    }

    /// Fit binary classification directly
    fn fit_binary(&mut self, x: &Array2<f64>, y: &Array1<f64>) -> Result<()> {
        let y_binary: Array1<f64> = y.mapv(|v| {
            if v.round() as i64 == self.classes[1] { 1.0 } else { -1.0 }
        });

        let (alphas, bias, support_indices) = self.smo_train(x, &y_binary)?;

        let sv_count = support_indices.len();
        let n_features = x.ncols();

        let mut support_vectors = Array2::zeros((sv_count, n_features));
        let mut support_labels = Array1::zeros(sv_count);
        let mut support_alphas = Array1::zeros(sv_count);

        for (i, &idx) in support_indices.iter().enumerate() {
            support_vectors.row_mut(i).assign(&x.row(idx));
            support_labels[i] = y_binary[idx];
            support_alphas[i] = alphas[idx];
        }

        self.support_vectors = Some(support_vectors);
        self.support_labels = Some(support_labels);
        self.alphas = Some(support_alphas);
        self.bias = bias;
        Ok(())
    }

    /// Fit multi-class classification using One-vs-Rest strategy
    fn fit_ovr(&mut self, x: &Array2<f64>, y: &Array1<f64>) -> Result<()> {
        self.ovr_classifiers.clear();

        for &cls in &self.classes {
            let y_binary: Array1<f64> = y.mapv(|v| {
                if v.round() as i64 == cls { 1.0 } else { -1.0 }
            });

            let (alphas, bias, support_indices) = self.smo_train(x, &y_binary)?;

            let sv_count = support_indices.len();
            let n_features = x.ncols();

            let mut support_vectors = Array2::zeros((sv_count, n_features));
            let mut support_labels = Array1::zeros(sv_count);
            let mut support_alphas = Array1::zeros(sv_count);

            for (i, &idx) in support_indices.iter().enumerate() {
                support_vectors.row_mut(i).assign(&x.row(idx));
                support_labels[i] = y_binary[idx];
                support_alphas[i] = alphas[idx];
            }

            self.ovr_classifiers.push(BinarySVM {
                support_vectors,
                alphas: support_alphas,
                support_labels,
                bias,
            });
        }

        Ok(())
    }

    /// SMO training algorithm
    fn smo_train(
        &self,
        x: &Array2<f64>,
        y: &Array1<f64>,
    ) -> Result<(Array1<f64>, f64, Vec<usize>)> {
        let n = x.nrows();

        if n > MAX_KERNEL_MATRIX_SAMPLES {
            return Err(KolosalError::InvalidInput(
                format!(
                    "Dataset has {} samples, exceeding the maximum {} for SVM kernel matrix. \
                     Consider subsampling or using a different algorithm.",
                    n, MAX_KERNEL_MATRIX_SAMPLES
                )
            ));
        }

        let mut alphas = Array1::zeros(n);
        let mut bias = 0.0;

        // Precompute kernel matrix for efficiency
        let kernel_matrix = self.compute_kernel_matrix(x);

        let mut rng = match self.config.random_state {
            Some(seed) => Xoshiro256PlusPlus::seed_from_u64(seed),
            None => Xoshiro256PlusPlus::from_entropy(),
        };

        let mut passes = 0;
        let max_passes = 5;
        let mut total_iter = 0;

        while passes < max_passes && total_iter < self.config.max_iter {
            let mut num_changed = 0;

            if n <= 1 {
                break;
            }

            for i in 0..n {
                // Calculate error for i
                let e_i = self.decision_function_cached(&kernel_matrix, &alphas, y, bias, i) - y[i];

                // Check KKT conditions
                if (y[i] * e_i < -self.config.tol && alphas[i] < self.config.c)
                    || (y[i] * e_i > self.config.tol && alphas[i] > 0.0)
                {
                    // Select j randomly (safe: n > 1 guaranteed above)
                    let j = loop {
                        let j = rng.gen_range(0..n);
                        if j != i { break j; }
                    };
                    
                    let e_j = self.decision_function_cached(&kernel_matrix, &alphas, y, bias, j) - y[j];
                    
                    let alpha_i_old = alphas[i];
                    let alpha_j_old = alphas[j];
                    
                    // Compute bounds
                    let (l, h) = if y[i] != y[j] {
                        (
                            (alphas[j] - alphas[i]).max(0.0),
                            (self.config.c + alphas[j] - alphas[i]).min(self.config.c)
                        )
                    } else {
                        (
                            (alphas[i] + alphas[j] - self.config.c).max(0.0),
                            (alphas[i] + alphas[j]).min(self.config.c)
                        )
                    };
                    
                    if (l - h).abs() < 1e-10 {
                        continue;
                    }
                    
                    // Compute eta
                    let eta = 2.0 * kernel_matrix[[i, j]] - kernel_matrix[[i, i]] - kernel_matrix[[j, j]];
                    
                    if eta >= 0.0 {
                        continue;
                    }
                    
                    // Update alpha_j
                    alphas[j] = alphas[j] - y[j] * (e_i - e_j) / eta;
                    alphas[j] = alphas[j].max(l).min(h);
                    
                    if (alphas[j] - alpha_j_old).abs() < 1e-5 {
                        continue;
                    }
                    
                    // Update alpha_i
                    alphas[i] = alphas[i] + y[i] * y[j] * (alpha_j_old - alphas[j]);
                    
                    // Update bias
                    let b1 = bias - e_i
                        - y[i] * (alphas[i] - alpha_i_old) * kernel_matrix[[i, i]]
                        - y[j] * (alphas[j] - alpha_j_old) * kernel_matrix[[i, j]];
                    
                    let b2 = bias - e_j
                        - y[i] * (alphas[i] - alpha_i_old) * kernel_matrix[[i, j]]
                        - y[j] * (alphas[j] - alpha_j_old) * kernel_matrix[[j, j]];
                    
                    bias = if alphas[i] > 0.0 && alphas[i] < self.config.c {
                        b1
                    } else if alphas[j] > 0.0 && alphas[j] < self.config.c {
                        b2
                    } else {
                        (b1 + b2) / 2.0
                    };
                    
                    num_changed += 1;
                }
            }
            
            total_iter += 1;
            if num_changed == 0 {
                passes += 1;
            } else {
                passes = 0;
            }
        }
        
        // Find support vectors (alpha > 0)
        let support_indices: Vec<usize> = alphas
            .iter()
            .enumerate()
            .filter(|(_, &a)| a > 1e-8)
            .map(|(i, _)| i)
            .collect();
        
        Ok((alphas, bias, support_indices))
    }

    /// Compute kernel matrix (parallelized for large datasets)
    fn compute_kernel_matrix(&self, x: &Array2<f64>) -> Array2<f64> {
        let n = x.nrows();

        // For small matrices, sequential is faster due to overhead
        if n < 100 {
            let mut k = Array2::zeros((n, n));
            for i in 0..n {
                for j in i..n {
                    let val = self.kernel(&x.row(i).to_owned(), &x.row(j).to_owned());
                    k[[i, j]] = val;
                    k[[j, i]] = val;
                }
            }
            return k;
        }

        // Parallel: compute upper triangle rows in parallel
        let config = self.config.clone();
        let x_data: Vec<Vec<f64>> = (0..n)
            .map(|i| x.row(i).to_vec())
            .collect();

        let rows: Vec<Vec<(usize, f64)>> = (0..n)
            .into_par_iter()
            .map(|i| {
                let row_i = &x_data[i];
                let a = Array1::from_vec(row_i.clone());
                (i..n)
                    .map(|j| {
                        let b = Array1::from_vec(x_data[j].clone());
                        let val = match &config.kernel {
                            KernelType::Linear => a.dot(&b),
                            KernelType::Polynomial { degree, gamma, coef0 } => {
                                (*gamma * a.dot(&b) + coef0).powi((*degree).min(i32::MAX as usize) as i32)
                            }
                            KernelType::RBF { gamma } => {
                                let diff = &a - &b;
                                (-gamma * diff.dot(&diff)).exp()
                            }
                            KernelType::Sigmoid { gamma, coef0 } => {
                                (*gamma * a.dot(&b) + coef0).tanh()
                            }
                        };
                        (j, val)
                    })
                    .collect()
            })
            .collect();

        let mut k = Array2::zeros((n, n));
        for (i, row_vals) in rows.into_iter().enumerate() {
            for (j, val) in row_vals {
                k[[i, j]] = val;
                k[[j, i]] = val;
            }
        }
        k
    }

    /// Compute kernel between two vectors
    fn kernel(&self, x1: &Array1<f64>, x2: &Array1<f64>) -> f64 {
        match &self.config.kernel {
            KernelType::Linear => x1.dot(x2),
            KernelType::Polynomial { degree, gamma, coef0 } => {
                (*gamma * x1.dot(x2) + coef0).powi((*degree).min(i32::MAX as usize) as i32)
            }
            KernelType::RBF { gamma } => {
                let diff = x1 - x2;
                let norm_sq = diff.dot(&diff);
                (-gamma * norm_sq).exp()
            }
            KernelType::Sigmoid { gamma, coef0 } => {
                (*gamma * x1.dot(x2) + coef0).tanh()
            }
        }
    }

    /// Decision function using cached kernel matrix
    fn decision_function_cached(
        &self,
        k: &Array2<f64>,
        alphas: &Array1<f64>,
        y: &Array1<f64>,
        bias: f64,
        idx: usize,
    ) -> f64 {
        let mut sum = 0.0;
        for i in 0..alphas.len() {
            sum += alphas[i] * y[i] * k[[i, idx]];
        }
        sum + bias
    }

    /// Compute the decision function score for a single sample using given SVM parameters
    fn score_sample(
        &self,
        sample: &Array1<f64>,
        sv: &Array2<f64>,
        alphas: &Array1<f64>,
        sv_labels: &Array1<f64>,
        bias: f64,
    ) -> f64 {
        let mut sum = bias;
        for j in 0..sv.nrows() {
            let k_val = self.kernel(sample, &sv.row(j).to_owned());
            sum += alphas[j] * sv_labels[j] * k_val;
        }
        sum
    }

    /// Predict class labels (binary and multi-class)
    pub fn predict(&self, x: &Array2<f64>) -> Result<Array1<f64>> {
        if !self.is_fitted {
            return Err(KolosalError::ModelNotFitted);
        }

        let n = x.nrows();
        let mut predictions = Array1::zeros(n);

        if self.classes.len() == 2 {
            let sv = self.support_vectors.as_ref().unwrap();
            let sv_labels = self.support_labels.as_ref().unwrap();
            let alphas = self.alphas.as_ref().unwrap();

            for i in 0..n {
                let sample = x.row(i).to_owned();
                let score = self.score_sample(&sample, sv, alphas, sv_labels, self.bias);
                predictions[i] = if score >= 0.0 {
                    self.classes[1] as f64
                } else {
                    self.classes[0] as f64
                };
            }
        } else {
            for i in 0..n {
                let sample = x.row(i).to_owned();
                let mut best_score = f64::NEG_INFINITY;
                let mut best_class = self.classes[0];

                for (k, clf) in self.ovr_classifiers.iter().enumerate() {
                    let score = self.score_sample(
                        &sample,
                        &clf.support_vectors,
                        &clf.alphas,
                        &clf.support_labels,
                        clf.bias,
                    );
                    if score > best_score {
                        best_score = score;
                        best_class = self.classes[k];
                    }
                }

                predictions[i] = best_class as f64;
            }
        }

        Ok(predictions)
    }

    /// Get decision function values (binary: single score, multi-class: max OvR score)
    pub fn decision_function(&self, x: &Array2<f64>) -> Result<Array1<f64>> {
        if !self.is_fitted {
            return Err(KolosalError::ModelNotFitted);
        }

        let n = x.nrows();
        let mut scores = Array1::zeros(n);

        if self.classes.len() == 2 {
            let sv = self.support_vectors.as_ref().unwrap();
            let sv_labels = self.support_labels.as_ref().unwrap();
            let alphas = self.alphas.as_ref().unwrap();

            for i in 0..n {
                let sample = x.row(i).to_owned();
                scores[i] = self.score_sample(&sample, sv, alphas, sv_labels, self.bias);
            }
        } else {
            for i in 0..n {
                let sample = x.row(i).to_owned();
                let mut best_score = f64::NEG_INFINITY;
                for clf in &self.ovr_classifiers {
                    let score = self.score_sample(
                        &sample,
                        &clf.support_vectors,
                        &clf.alphas,
                        &clf.support_labels,
                        clf.bias,
                    );
                    if score > best_score {
                        best_score = score;
                    }
                }
                scores[i] = best_score;
            }
        }

        Ok(scores)
    }

    /// Get number of support vectors
    pub fn n_support_vectors(&self) -> usize {
        self.support_vectors.as_ref().map(|sv| sv.nrows()).unwrap_or(0)
    }
}

/// Support Vector Regressor
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SVMRegressor {
    config: SVMConfig,
    support_vectors: Option<Array2<f64>>,
    alphas: Option<Array1<f64>>,  // alpha - alpha*
    bias: f64,
    is_fitted: bool,
}

impl SVMRegressor {
    /// Create a new SVM regressor
    pub fn new(config: SVMConfig) -> Self {
        Self {
            config,
            support_vectors: None,
            alphas: None,
            bias: 0.0,
            is_fitted: false,
        }
    }

    /// Fit the regressor using gradient descent on epsilon-insensitive loss
    pub fn fit(&mut self, x: &Array2<f64>, y: &Array1<f64>) -> Result<()> {
        let n = x.nrows();

        if n > MAX_KERNEL_MATRIX_SAMPLES {
            return Err(KolosalError::InvalidInput(
                format!(
                    "Dataset has {} samples, exceeding the maximum {} for SVR kernel matrix. \
                     Consider subsampling or using a different algorithm.",
                    n, MAX_KERNEL_MATRIX_SAMPLES
                )
            ));
        }

        let mut alphas: Array1<f64> = Array1::zeros(n);      // alpha
        let mut alphas_star: Array1<f64> = Array1::zeros(n); // alpha*
        let mut bias: f64 = 0.0;

        let kernel_matrix = self.compute_kernel_matrix(x);

        // Gradient descent approach for SVR with convergence check
        let learning_rate: f64 = 0.01;

        for _iter in 0..self.config.max_iter {
            let mut max_change: f64 = 0.0;

            for i in 0..n {
                let mut pred: f64 = bias;
                for j in 0..n {
                    pred += (alphas[j] - alphas_star[j]) * kernel_matrix[[j, i]];
                }

                let error: f64 = pred - y[i];

                // Update alphas based on epsilon-insensitive loss
                if error > self.config.epsilon {
                    let new_val = (alphas_star[i] + learning_rate).min(self.config.c);
                    max_change = max_change.max((new_val - alphas_star[i]).abs());
                    alphas_star[i] = new_val;
                } else if error < -self.config.epsilon {
                    let new_val = (alphas[i] + learning_rate).min(self.config.c);
                    max_change = max_change.max((new_val - alphas[i]).abs());
                    alphas[i] = new_val;
                }

                // Update bias
                let bias_update = learning_rate * 0.1 * error;
                max_change = max_change.max(bias_update.abs());
                bias -= bias_update;
            }

            // Convergence check: stop if all updates are within tolerance
            if max_change < self.config.tol {
                break;
            }
        }
        
        // Combine into single alpha vector (alpha - alpha*)
        let combined_alphas = &alphas - &alphas_star;
        
        // Find support vectors
        let support_indices: Vec<usize> = combined_alphas
            .iter()
            .enumerate()
            .filter(|(_, a): &(usize, &f64)| a.abs() > 1e-8)
            .map(|(i, _)| i)
            .collect();
        
        let sv_count = support_indices.len();
        if sv_count == 0 {
            // Fallback: use all points if no support vectors found
            self.support_vectors = Some(x.clone());
            self.alphas = Some(combined_alphas);
        } else {
            let n_features = x.ncols();
            let mut support_vectors = Array2::zeros((sv_count, n_features));
            let mut support_alphas = Array1::zeros(sv_count);
            
            for (i, &idx) in support_indices.iter().enumerate() {
                support_vectors.row_mut(i).assign(&x.row(idx));
                support_alphas[i] = combined_alphas[idx];
            }
            
            self.support_vectors = Some(support_vectors);
            self.alphas = Some(support_alphas);
        }
        
        self.bias = bias;
        self.is_fitted = true;
        
        Ok(())
    }

    fn compute_kernel_matrix(&self, x: &Array2<f64>) -> Array2<f64> {
        let n = x.nrows();
        let mut k = Array2::zeros((n, n));
        
        for i in 0..n {
            for j in i..n {
                let val = self.kernel(&x.row(i).to_owned(), &x.row(j).to_owned());
                k[[i, j]] = val;
                k[[j, i]] = val;
            }
        }
        
        k
    }

    fn kernel(&self, x1: &Array1<f64>, x2: &Array1<f64>) -> f64 {
        match &self.config.kernel {
            KernelType::Linear => x1.dot(x2),
            KernelType::Polynomial { degree, gamma, coef0 } => {
                (*gamma * x1.dot(x2) + coef0).powi((*degree).min(i32::MAX as usize) as i32)
            }
            KernelType::RBF { gamma } => {
                let diff = x1 - x2;
                let norm_sq = diff.dot(&diff);
                (-gamma * norm_sq).exp()
            }
            KernelType::Sigmoid { gamma, coef0 } => {
                (*gamma * x1.dot(x2) + coef0).tanh()
            }
        }
    }

    /// Predict target values
    pub fn predict(&self, x: &Array2<f64>) -> Result<Array1<f64>> {
        if !self.is_fitted {
            return Err(KolosalError::ModelNotFitted);
        }
        
        let sv = self.support_vectors.as_ref().unwrap();
        let alphas = self.alphas.as_ref().unwrap();
        
        let n = x.nrows();
        let mut predictions = Array1::zeros(n);
        
        for i in 0..n {
            let sample = x.row(i).to_owned();
            let mut sum = self.bias;
            
            for j in 0..sv.nrows() {
                let k_val = self.kernel(&sample, &sv.row(j).to_owned());
                sum += alphas[j] * k_val;
            }
            
            predictions[i] = sum;
        }
        
        Ok(predictions)
    }

    /// Get number of support vectors
    pub fn n_support_vectors(&self) -> usize {
        self.support_vectors.as_ref().map(|sv| sv.nrows()).unwrap_or(0)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn create_linear_separable_data() -> (Array2<f64>, Array1<f64>) {
        // Simple linearly separable data
        let x = Array2::from_shape_vec((10, 2), vec![
            1.0, 1.0,
            1.5, 1.2,
            2.0, 2.0,
            1.2, 1.8,
            0.8, 1.5,
            5.0, 5.0,
            5.5, 5.2,
            6.0, 6.0,
            5.2, 5.8,
            4.8, 5.5,
        ]).unwrap();
        
        let y = Array1::from_vec(vec![0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0]);
        
        (x, y)
    }

    #[test]
    fn test_svm_classifier_linear() {
        let (x, y) = create_linear_separable_data();
        
        let config = SVMConfig {
            c: 1.0,
            kernel: KernelType::Linear,
            max_iter: 1000,
            ..Default::default()
        };
        
        let mut svm = SVMClassifier::new(config);
        svm.fit(&x, &y).unwrap();
        
        let predictions = svm.predict(&x).unwrap();
        
        // Should correctly classify most points
        let correct: usize = y.iter()
            .zip(predictions.iter())
            .filter(|(&yi, &pi)| yi == pi)
            .count();
        
        let accuracy = correct as f64 / y.len() as f64;
        assert!(accuracy > 0.8, "Accuracy {} should be > 0.8", accuracy);
    }

    #[test]
    fn test_svm_classifier_rbf() {
        let (x, y) = create_linear_separable_data();
        
        let config = SVMConfig {
            c: 1.0,
            kernel: KernelType::RBF { gamma: 0.5 },
            max_iter: 1000,
            ..Default::default()
        };
        
        let mut svm = SVMClassifier::new(config);
        svm.fit(&x, &y).unwrap();
        
        let predictions = svm.predict(&x).unwrap();
        assert_eq!(predictions.len(), 10);
    }

    #[test]
    fn test_svm_classifier_multiclass() {
        // 3-class data
        let x = Array2::from_shape_vec((15, 2), vec![
            1.0, 1.0,  1.5, 1.2,  2.0, 2.0,  1.2, 1.8,  0.8, 1.5,
            5.0, 5.0,  5.5, 5.2,  6.0, 6.0,  5.2, 5.8,  4.8, 5.5,
            1.0, 5.0,  1.5, 5.2,  2.0, 6.0,  1.2, 5.8,  0.8, 5.5,
        ]).unwrap();

        let y = Array1::from_vec(vec![
            0.0, 0.0, 0.0, 0.0, 0.0,
            1.0, 1.0, 1.0, 1.0, 1.0,
            2.0, 2.0, 2.0, 2.0, 2.0,
        ]);

        let config = SVMConfig {
            c: 10.0,
            kernel: KernelType::RBF { gamma: 0.5 },
            max_iter: 1000,
            ..Default::default()
        };

        let mut svm = SVMClassifier::new(config);
        svm.fit(&x, &y).unwrap();

        let predictions = svm.predict(&x).unwrap();
        assert_eq!(predictions.len(), 15);

        // All predictions should be one of the 3 classes
        for &p in predictions.iter() {
            assert!(p == 0.0 || p == 1.0 || p == 2.0, "Unexpected class: {}", p);
        }

        // Should get reasonable accuracy on training data
        let correct: usize = y.iter()
            .zip(predictions.iter())
            .filter(|(&yi, &pi)| yi == pi)
            .count();
        let accuracy = correct as f64 / y.len() as f64;
        assert!(accuracy > 0.6, "Multi-class accuracy {} should be > 0.6", accuracy);
    }

    #[test]
    fn test_svm_regressor() {
        // Simple regression data
        let x = Array2::from_shape_vec((10, 1), vec![
            1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0
        ]).unwrap();
        
        let y = Array1::from_vec(vec![2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0, 18.0, 20.0]);
        
        let config = SVMConfig {
            c: 10.0,
            kernel: KernelType::Linear,
            epsilon: 0.5,
            max_iter: 500,
            ..Default::default()
        };
        
        let mut svr = SVMRegressor::new(config);
        svr.fit(&x, &y).unwrap();
        
        let predictions = svr.predict(&x).unwrap();
        
        // Check predictions are reasonable (within 20% of actual)
        for (pred, actual) in predictions.iter().zip(y.iter()) {
            let error = (pred - actual).abs() / actual;
            assert!(error < 0.5, "Error {} too large for pred={}, actual={}", error, pred, actual);
        }
    }
}
