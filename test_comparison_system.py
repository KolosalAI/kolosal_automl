#!/usr/bin/env python3
"""
Test Comparison System

This script tests the comparison system setup and validates that all components work correctly.
"""

import os
import sys
import subprocess
import tempfile
import json
from pathlib import Path

def test_imports():
    """Test that all required modules can be imported."""
    print("üîç Testing imports...")
    
    try:
        import numpy as np
        import pandas as pd
        import sklearn
        import matplotlib
        import psutil
        print("‚úÖ Standard dependencies imported successfully")
    except ImportError as e:
        print(f"‚ùå Failed to import standard dependencies: {e}")
        return False
    
    try:
        # Test project imports
        project_root = Path(__file__).parent
        sys.path.insert(0, str(project_root))
        
        from benchmark.standard_ml_benchmark import StandardMLBenchmark
        from benchmark.kolosal_automl_benchmark import KolosalAutoMLBenchmark
        print("‚úÖ Benchmark modules imported successfully")
    except ImportError as e:
        print(f"‚ö†Ô∏è  Benchmark modules import failed (expected if modules not available): {e}")
    
    return True

def test_scripts_exist():
    """Test that all script files exist."""
    print("\nüìÅ Testing script files...")
    
    project_root = Path(__file__).parent
    required_files = [
        "run_kolosal_comparison.py",
        "run_standard_ml.py", 
        "run_kolosal_automl.py",
        "benchmark/standard_ml_benchmark.py",
        "benchmark/kolosal_automl_benchmark.py",
        "benchmark/configs/example_comparison_config.json"
    ]
    
    all_exist = True
    for file_path in required_files:
        full_path = project_root / file_path
        if full_path.exists():
            print(f"‚úÖ {file_path}")
        else:
            print(f"‚ùå {file_path} - Missing")
            all_exist = False
    
    return all_exist

def test_standard_ml_benchmark():
    """Test standard ML benchmark with minimal dataset."""
    print("\nüß™ Testing Standard ML benchmark...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        script_path = Path(__file__).parent / "benchmark" / "standard_ml_benchmark.py"
        cmd = [
            sys.executable, str(script_path),
            "--output-dir", temp_dir,
            "--datasets", "iris",
            "--models", "random_forest",
            "--optimization", "random_search"
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            if result.returncode == 0:
                print("‚úÖ Standard ML benchmark completed successfully")
                
                # Check for output files
                output_files = list(Path(temp_dir).glob("*.json"))
                if output_files:
                    print(f"‚úÖ Generated {len(output_files)} output files")
                    return True
                else:
                    print("‚ö†Ô∏è  No output files generated")
                    return False
            else:
                print(f"‚ùå Standard ML benchmark failed: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            print("‚ùå Standard ML benchmark timed out")
            return False
        except Exception as e:
            print(f"‚ùå Error running Standard ML benchmark: {e}")
            return False

def test_kolosal_automl_benchmark():
    """Test Kolosal AutoML benchmark with minimal dataset."""
    print("\nüöÄ Testing Kolosal AutoML benchmark...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        script_path = Path(__file__).parent / "benchmark" / "kolosal_automl_benchmark.py"
        cmd = [
            sys.executable, str(script_path),
            "--output-dir", temp_dir,
            "--datasets", "iris",
            "--models", "random_forest", 
            "--optimization", "random_search"
        ]
        
        try:
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            if result.returncode == 0:
                print("‚úÖ Kolosal AutoML benchmark completed successfully")
                
                # Check for output files
                output_files = list(Path(temp_dir).glob("*.json"))
                if output_files:
                    print(f"‚úÖ Generated {len(output_files)} output files")
                    return True
                else:
                    print("‚ö†Ô∏è  No output files generated")
                    return False
            else:
                print(f"‚ùå Kolosal AutoML benchmark failed: {result.stderr}")
                print(f"‚ÑπÔ∏è  This is expected if Kolosal AutoML modules are not available")
                return True  # Not a failure if modules aren't available
                
        except subprocess.TimeoutExpired:
            print("‚ùå Kolosal AutoML benchmark timed out")
            return False
        except Exception as e:
            print(f"‚ùå Error running Kolosal AutoML benchmark: {e}")
            return False

def test_comparison_runner():
    """Test the main comparison runner."""
    print("\nüîÑ Testing comparison runner...")
    
    # Test help command
    script_path = Path(__file__).parent / "run_kolosal_comparison.py"
    cmd = [sys.executable, str(script_path), "--help"]
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        if result.returncode == 0 and "usage:" in result.stdout.lower():
            print("‚úÖ Comparison runner help works")
            return True
        else:
            print(f"‚ùå Comparison runner help failed: {result.stderr}")
            return False
    except Exception as e:
        print(f"‚ùå Error testing comparison runner: {e}")
        return False

def test_configuration_file():
    """Test configuration file loading."""
    print("\n‚öôÔ∏è  Testing configuration file...")
    
    config_path = Path(__file__).parent / "benchmark" / "configs" / "example_comparison_config.json"
    
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
        
        required_keys = ["datasets", "models", "optimization"]
        if all(key in config for key in required_keys):
            print("‚úÖ Configuration file is valid")
            print(f"   Datasets: {len(config['datasets'])}")
            print(f"   Models: {len(config['models'])}")
            print(f"   Optimization: {config['optimization']}")
            return True
        else:
            print("‚ùå Configuration file missing required keys")
            return False
            
    except Exception as e:
        print(f"‚ùå Error reading configuration file: {e}")
        return False

def main():
    """Run all tests."""
    print("üß™ Testing Kolosal AutoML Comparison System")
    print("=" * 50)
    
    tests = [
        ("Dependencies", test_imports),
        ("Script Files", test_scripts_exist), 
        ("Configuration", test_configuration_file),
        ("Comparison Runner", test_comparison_runner),
        ("Standard ML Benchmark", test_standard_ml_benchmark),
        ("Kolosal AutoML Benchmark", test_kolosal_automl_benchmark)
    ]
    
    results = []
    for test_name, test_func in tests:
        try:
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå Test '{test_name}' crashed: {e}")
            results.append((test_name, False))
    
    # Summary
    print("\n" + "=" * 50)
    print("üìä Test Results Summary:")
    
    passed = 0
    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"  {status} {test_name}")
        if result:
            passed += 1
    
    print(f"\nüéØ Overall: {passed}/{len(results)} tests passed")
    
    if passed == len(results):
        print("üéâ All tests passed! The comparison system is ready to use.")
        print("\nTo run a quick comparison:")
        print("python run_kolosal_comparison.py --mode quick")
    else:
        print("‚ö†Ô∏è  Some tests failed. Check the output above for details.")
        print("\nThe system may still work for available components.")
    
    return 0 if passed == len(results) else 1

if __name__ == "__main__":
    sys.exit(main())
